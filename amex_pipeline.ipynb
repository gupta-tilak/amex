{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed540d6a",
   "metadata": {},
   "source": [
    "# Amex Pipeline: Staged Execution\n",
    "This notebook allows you to run the Amex pipeline in stages: data loading, cleaning, feature engineering, EDA, feature selection, model training, validation, and submission generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05a78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data.data_loader import load_all_data\n",
    "from data.data_cleaning import clean_all_data_advanced\n",
    "from data.advanced_feature_engineering import create_full_feature_set_advanced\n",
    "from eda.exploratory_analysis import (\n",
    "    plot_target_distribution, plot_missing_values, plot_feature_distributions, \n",
    "    plot_correlation_heatmap, plot_new_feature_analysis\n",
    ")\n",
    "from utils.metrics import map7_from_dataframe\n",
    "from utils.submission import generate_submission\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def validate_pipeline_data(data, stage_name):\n",
    "    \"\"\"Validate data at each pipeline stage\"\"\"\n",
    "    print(f\"\\n=== {stage_name} Validation ===\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    \n",
    "    # Check for NaN values\n",
    "    nan_count = data.isna().sum().sum()\n",
    "    print(f\"NaN values: {nan_count}\")\n",
    "    \n",
    "    # Check for infinite values\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    inf_count = np.isinf(data[numeric_cols]).sum().sum()\n",
    "    print(f\"Infinite values: {inf_count}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_mb = data.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    print(f\"Memory usage: {memory_mb:.1f} MB\")\n",
    "    \n",
    "    if nan_count > 0 or inf_count > 0:\n",
    "        print(\"⚠️  Data quality issues detected!\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"✅ Data validation passed\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a46ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 1: DATA LOADING ===\n",
      "✅ Data loaded successfully\n",
      "Train shape: (770164, 372)\n",
      "Test shape: (369301, 371)\n",
      "\n",
      "=== Raw Train Data Validation ===\n",
      "Shape: (770164, 372)\n",
      "NaN values: 68296406\n",
      "Infinite values: 0\n",
      "Memory usage: 1084.8 MB\n",
      "⚠️  Data quality issues detected!\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Load data with validation\n",
    "print(\"=== STAGE 1: DATA LOADING ===\")\n",
    "try:\n",
    "    data = load_all_data()\n",
    "    print(f\"✅ Data loaded successfully\")\n",
    "    print(f\"Train shape: {data['train'].shape}\")\n",
    "    print(f\"Test shape: {data['test'].shape}\")\n",
    "    \n",
    "    # Validate loaded data\n",
    "    validate_pipeline_data(data['train'], \"Raw Train Data\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Data loading failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e418c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 2: ADVANCED DATA CLEANING ===\n",
      "Starting robust advanced data cleaning pipeline...\n",
      "Starting robust advanced data cleaning...\n",
      "Starting robust advanced data cleaning...\n",
      "Removed 0 duplicate rows\n",
      "Removed 0 duplicate rows\n",
      "Creating focused customer behavioral features with priority features...\n",
      "Creating focused customer behavioral features with priority features...\n",
      "Creating priority customer aggregations...\n",
      "Creating priority customer aggregations...\n",
      "Created customer aggregations for 7 priority features\n",
      "Creating customer segments with priority features...\n",
      "Created customer aggregations for 7 priority features\n",
      "Creating customer segments with priority features...\n",
      "Created 3 customer segments with 8 priority features\n",
      "Created 3 customer segments with 8 priority features\n",
      "Performing focused imputation with priority features...\n",
      "Performing focused imputation with priority features...\n",
      "Phase 1: Priority features imputation...\n",
      "Phase 2: Simplified imputation for remaining features...\n",
      "Applying focused imputation strategy...\n",
      "Imputing 491658 missing values in f1\n",
      "Phase 1: Priority features imputation...\n",
      "Phase 2: Simplified imputation for remaining features...\n",
      "Applying focused imputation strategy...\n",
      "Imputing 491658 missing values in f1\n",
      "Imputing 447192 missing values in f2\n",
      "Imputing 661602 missing values in f3\n",
      "Imputing 447192 missing values in f2\n",
      "Imputing 661602 missing values in f3\n",
      "Imputing 701295 missing values in f4\n",
      "Imputing 231810 missing values in f5\n",
      "Imputing 701295 missing values in f4\n",
      "Imputing 231810 missing values in f5\n",
      "Imputing 150109 missing values in f6\n",
      "Imputing 150109 missing values in f6\n",
      "Imputing 367438 missing values in f7\n",
      "Imputing 367438 missing values in f7\n",
      "Imputing 234515 missing values in f8\n",
      "Imputing 234515 missing values in f8\n",
      "Imputing 284258 missing values in f9\n",
      "Imputing 284258 missing values in f9\n",
      "Imputing 253665 missing values in f10\n",
      "Imputing 253665 missing values in f10\n",
      "Imputing 356562 missing values in f11\n",
      "Imputing 356562 missing values in f11\n",
      "Imputing 242372 missing values in f12\n",
      "Imputing 242372 missing values in f12\n",
      "Imputing 769468 missing values in f13\n",
      "Imputing 769468 missing values in f13\n",
      "Imputing 769468 missing values in f14\n",
      "Imputing 769468 missing values in f14\n",
      "Imputing 769468 missing values in f15\n",
      "Imputing 769468 missing values in f15\n",
      "Phase 3: Minimal categorical feature imputation...\n",
      "Phase 3: Minimal categorical feature imputation...\n",
      "Robust advanced cleaning completed. Final shape: (770164, 380)\n",
      "Starting robust advanced data cleaning...\n",
      "Robust advanced cleaning completed. Final shape: (770164, 380)\n",
      "Starting robust advanced data cleaning...\n",
      "Removed 0 duplicate rows\n",
      "Removed 0 duplicate rows\n",
      "Creating focused customer behavioral features with priority features...\n",
      "Creating focused customer behavioral features with priority features...\n",
      "Creating priority customer aggregations...\n",
      "Created customer aggregations for 7 priority features\n",
      "Creating customer segments with priority features...\n",
      "Creating priority customer aggregations...\n",
      "Created customer aggregations for 7 priority features\n",
      "Creating customer segments with priority features...\n",
      "Created 3 customer segments with 8 priority features\n",
      "Created 3 customer segments with 8 priority features\n",
      "Performing focused imputation with priority features...\n",
      "Performing focused imputation with priority features...\n",
      "Phase 1: Priority features imputation...\n",
      "Phase 2: Simplified imputation for remaining features...\n",
      "Applying focused imputation strategy...\n",
      "Imputing 237422 missing values in f1\n",
      "Imputing 214285 missing values in f2\n",
      "Imputing 319554 missing values in f3\n",
      "Imputing 338807 missing values in f4\n",
      "Phase 1: Priority features imputation...\n",
      "Phase 2: Simplified imputation for remaining features...\n",
      "Applying focused imputation strategy...\n",
      "Imputing 237422 missing values in f1\n",
      "Imputing 214285 missing values in f2\n",
      "Imputing 319554 missing values in f3\n",
      "Imputing 338807 missing values in f4\n",
      "Imputing 112287 missing values in f5\n",
      "Imputing 74264 missing values in f6\n",
      "Imputing 112287 missing values in f5\n",
      "Imputing 74264 missing values in f6\n",
      "Imputing 176785 missing values in f7\n",
      "Imputing 116271 missing values in f8\n",
      "Imputing 139140 missing values in f9\n",
      "Imputing 176785 missing values in f7\n",
      "Imputing 116271 missing values in f8\n",
      "Imputing 139140 missing values in f9\n",
      "Imputing 119899 missing values in f10\n",
      "Imputing 171966 missing values in f11\n",
      "Imputing 116307 missing values in f12\n",
      "Imputing 119899 missing values in f10\n",
      "Imputing 171966 missing values in f11\n",
      "Imputing 116307 missing values in f12\n",
      "Imputing 368853 missing values in f13\n",
      "Imputing 368853 missing values in f14\n",
      "Imputing 368853 missing values in f13\n",
      "Imputing 368853 missing values in f14\n",
      "Imputing 368853 missing values in f15\n",
      "Phase 3: Minimal categorical feature imputation...\n",
      "Imputing 368853 missing values in f15\n",
      "Phase 3: Minimal categorical feature imputation...\n",
      "Robust advanced cleaning completed. Final shape: (369301, 379)\n",
      "✅ Advanced cleaning completed\n",
      "Cleaned train shape: (770164, 380)\n",
      "\n",
      "=== Cleaned Train Data Validation ===\n",
      "Shape: (770164, 380)\n",
      "Robust advanced cleaning completed. Final shape: (369301, 379)\n",
      "✅ Advanced cleaning completed\n",
      "Cleaned train shape: (770164, 380)\n",
      "\n",
      "=== Cleaned Train Data Validation ===\n",
      "Shape: (770164, 380)\n",
      "NaN values: 0\n",
      "NaN values: 0\n",
      "Infinite values: 0\n",
      "Infinite values: 0\n",
      "Memory usage: 1815.9 MB\n",
      "✅ Data validation passed\n",
      "Memory usage: 1815.9 MB\n",
      "✅ Data validation passed\n"
     ]
    }
   ],
   "source": [
    "# Stage 2: Advanced cleaning with comprehensive error handling\n",
    "print(\"\\n=== STAGE 2: ADVANCED DATA CLEANING ===\")\n",
    "try:\n",
    "    cleaned_data = clean_all_data_advanced(data)\n",
    "    print(f\"✅ Advanced cleaning completed\")\n",
    "    print(f\"Cleaned train shape: {cleaned_data['train'].shape}\")\n",
    "    \n",
    "    # Validate cleaned data\n",
    "    is_valid = validate_pipeline_data(cleaned_data['train'], \"Cleaned Train Data\")\n",
    "    \n",
    "    if not is_valid:\n",
    "        print(\"⚠️  Performing emergency data cleanup...\")\n",
    "        # Emergency cleanup\n",
    "        numeric_cols = cleaned_data['train'].select_dtypes(include=[np.number]).columns\n",
    "        cleaned_data['train'][numeric_cols] = cleaned_data['train'][numeric_cols].fillna(0)\n",
    "        cleaned_data['train'][numeric_cols] = cleaned_data['train'][numeric_cols].replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        cleaned_data['test'][numeric_cols] = cleaned_data['test'][numeric_cols].fillna(0)\n",
    "        cleaned_data['test'][numeric_cols] = cleaned_data['test'][numeric_cols].replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        print(\"✅ Emergency cleanup completed\")\n",
    "        validate_pipeline_data(cleaned_data['train'], \"Emergency Cleaned Data\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Data cleaning failed: {e}\")\n",
    "    print(\"Falling back to basic cleaning...\")\n",
    "    \n",
    "    # Emergency fallback\n",
    "    cleaned_data = {\n",
    "        'train': data['train'].fillna(0),\n",
    "        'test': data['test'].fillna(0)\n",
    "    }\n",
    "    print(\"✅ Basic cleaning completed as fallback\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9247b0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 3: ADVANCED FEATURE ENGINEERING ===\n",
      "Validating input data...\n",
      "\n",
      "=== Pre-Feature Engineering Validation ===\n",
      "Shape: (770164, 380)\n",
      "NaN values: 0\n",
      "NaN values: 0\n",
      "Infinite values: 0\n",
      "Infinite values: 0\n",
      "Memory usage: 1815.9 MB\n",
      "✅ Data validation passed\n",
      "Starting feature engineering...\n",
      "Starting advanced feature engineering...\n",
      "\n",
      "=== Data Quality Check: Initial Input ===\n",
      "Memory usage: 1815.9 MB\n",
      "✅ Data validation passed\n",
      "Starting feature engineering...\n",
      "Starting advanced feature engineering...\n",
      "\n",
      "=== Data Quality Check: Initial Input ===\n",
      "Memory usage: 1815.9 MB\n",
      "✓ Data quality check passed for Initial Input\n",
      "Creating interaction features...\n",
      "Created 2 interaction features\n",
      "\n",
      "=== Data Quality Check: After Interaction Features ===\n",
      "Memory usage: 1815.9 MB\n",
      "✓ Data quality check passed for Initial Input\n",
      "Creating interaction features...\n",
      "Created 2 interaction features\n",
      "\n",
      "=== Data Quality Check: After Interaction Features ===\n",
      "Memory usage: 1821.8 MB\n",
      "✓ Data quality check passed for After Interaction Features\n",
      "Creating temporal features...\n",
      "Created 4 temporal features\n",
      "\n",
      "=== Data Quality Check: After Temporal Features ===\n",
      "Memory usage: 1821.8 MB\n",
      "✓ Data quality check passed for After Interaction Features\n",
      "Creating temporal features...\n",
      "Created 4 temporal features\n",
      "\n",
      "=== Data Quality Check: After Temporal Features ===\n",
      "Memory usage: 1839.4 MB\n",
      "✓ Data quality check passed for After Temporal Features\n",
      "Creating aggregated features...\n",
      "Created 0 aggregated features\n",
      "\n",
      "=== Data Quality Check: After Aggregated Features ===\n",
      "Memory usage: 1839.4 MB\n",
      "✓ Data quality check passed for After Temporal Features\n",
      "Creating aggregated features...\n",
      "Created 0 aggregated features\n",
      "\n",
      "=== Data Quality Check: After Aggregated Features ===\n",
      "Memory usage: 1839.4 MB\n",
      "✓ Data quality check passed for After Aggregated Features\n",
      "Performing emergency NaN cleanup...\n",
      "Memory usage: 1839.4 MB\n",
      "✓ Data quality check passed for After Aggregated Features\n",
      "Performing emergency NaN cleanup...\n",
      "Performing feature selection...\n",
      "\n",
      "=== Data Quality Check: Feature Selection Input ===\n",
      "Performing feature selection...\n",
      "\n",
      "=== Data Quality Check: Feature Selection Input ===\n",
      "Memory usage: 1839.4 MB\n",
      "✓ Data quality check passed for Feature Selection Input\n",
      "Memory usage: 1839.4 MB\n",
      "✓ Data quality check passed for Feature Selection Input\n",
      "Performing emergency NaN cleanup...\n",
      "Performing emergency NaN cleanup...\n",
      "Selected top 100 features\n",
      "Top 10 features:\n",
      "    feature  importance\n",
      "117    f118    0.209120\n",
      "17      f18    0.206758\n",
      "18      f19    0.206305\n",
      "20      f21    0.205947\n",
      "15      f16    0.205752\n",
      "19      f20    0.205453\n",
      "16      f17    0.205097\n",
      "113    f114    0.202030\n",
      "114    f115    0.183089\n",
      "220    f221    0.180139\n",
      "\n",
      "=== Data Quality Check: Final Output ===\n",
      "Selected top 100 features\n",
      "Top 10 features:\n",
      "    feature  importance\n",
      "117    f118    0.209120\n",
      "17      f18    0.206758\n",
      "18      f19    0.206305\n",
      "20      f21    0.205947\n",
      "15      f16    0.205752\n",
      "19      f20    0.205453\n",
      "16      f17    0.205097\n",
      "113    f114    0.202030\n",
      "114    f115    0.183089\n",
      "220    f221    0.180139\n",
      "\n",
      "=== Data Quality Check: Final Output ===\n",
      "Memory usage: 1839.4 MB\n",
      "✓ Data quality check passed for Final Output\n",
      "Advanced feature engineering completed.\n",
      "Total features created: 6\n",
      "Selected features: 100\n",
      "Starting advanced feature engineering...\n",
      "\n",
      "=== Data Quality Check: Initial Input ===\n",
      "Memory usage: 1839.4 MB\n",
      "✓ Data quality check passed for Final Output\n",
      "Advanced feature engineering completed.\n",
      "Total features created: 6\n",
      "Selected features: 100\n",
      "Starting advanced feature engineering...\n",
      "\n",
      "=== Data Quality Check: Initial Input ===\n",
      "Memory usage: 867.0 MB\n",
      "✓ Data quality check passed for Initial Input\n",
      "Creating interaction features...\n",
      "Created 2 interaction features\n",
      "\n",
      "=== Data Quality Check: After Interaction Features ===\n",
      "Memory usage: 867.0 MB\n",
      "✓ Data quality check passed for Initial Input\n",
      "Creating interaction features...\n",
      "Created 2 interaction features\n",
      "\n",
      "=== Data Quality Check: After Interaction Features ===\n",
      "Memory usage: 869.8 MB\n",
      "✓ Data quality check passed for After Interaction Features\n",
      "Creating temporal features...\n",
      "Created 4 temporal features\n",
      "\n",
      "=== Data Quality Check: After Temporal Features ===\n",
      "Memory usage: 869.8 MB\n",
      "✓ Data quality check passed for After Interaction Features\n",
      "Creating temporal features...\n",
      "Created 4 temporal features\n",
      "\n",
      "=== Data Quality Check: After Temporal Features ===\n",
      "Memory usage: 878.3 MB\n",
      "✓ Data quality check passed for After Temporal Features\n",
      "Creating aggregated features...\n",
      "Created 0 aggregated features\n",
      "\n",
      "=== Data Quality Check: After Aggregated Features ===\n",
      "Memory usage: 878.3 MB\n",
      "✓ Data quality check passed for After Temporal Features\n",
      "Creating aggregated features...\n",
      "Created 0 aggregated features\n",
      "\n",
      "=== Data Quality Check: After Aggregated Features ===\n",
      "Memory usage: 878.3 MB\n",
      "✓ Data quality check passed for After Aggregated Features\n",
      "Performing emergency NaN cleanup...\n",
      "Memory usage: 878.3 MB\n",
      "✓ Data quality check passed for After Aggregated Features\n",
      "Performing emergency NaN cleanup...\n",
      "Performing feature selection...\n",
      "\n",
      "=== Data Quality Check: Feature Selection Input ===\n",
      "Performing feature selection...\n",
      "\n",
      "=== Data Quality Check: Feature Selection Input ===\n",
      "Memory usage: 878.3 MB\n",
      "✓ Data quality check passed for Feature Selection Input\n",
      "Memory usage: 878.3 MB\n",
      "✓ Data quality check passed for Feature Selection Input\n",
      "Target column not found, returning all features\n",
      "\n",
      "=== Data Quality Check: Final Output ===\n",
      "Target column not found, returning all features\n",
      "\n",
      "=== Data Quality Check: Final Output ===\n",
      "Memory usage: 878.3 MB\n",
      "✓ Data quality check passed for Final Output\n",
      "Advanced feature engineering completed.\n",
      "Total features created: 6\n",
      "Selected features: 379\n",
      "✅ Feature engineering completed\n",
      "Final train shape: (770164, 386)\n",
      "Selected features: 100\n",
      "\n",
      "=== Engineered Train Data Validation ===\n",
      "Shape: (770164, 386)\n",
      "Memory usage: 878.3 MB\n",
      "✓ Data quality check passed for Final Output\n",
      "Advanced feature engineering completed.\n",
      "Total features created: 6\n",
      "Selected features: 379\n",
      "✅ Feature engineering completed\n",
      "Final train shape: (770164, 386)\n",
      "Selected features: 100\n",
      "\n",
      "=== Engineered Train Data Validation ===\n",
      "Shape: (770164, 386)\n",
      "NaN values: 0\n",
      "NaN values: 0\n",
      "Infinite values: 0\n",
      "Infinite values: 0\n",
      "Memory usage: 1839.4 MB\n",
      "✅ Data validation passed\n",
      "\n",
      "=== Feature Engineering Quality Report ===\n",
      "Memory usage: 1839.4 MB\n",
      "✅ Data validation passed\n",
      "\n",
      "=== Feature Engineering Quality Report ===\n",
      "Remaining -999 values: 58955751\n",
      "Data types: {dtype('float32'): 364, dtype('O'): 13, dtype('int32'): 3, string[python]: 2, dtype('<M8[ns]'): 2, dtype('int64'): 2}\n",
      "\n",
      "Top 10 selected features:\n",
      " 1. f118\n",
      " 2. f18\n",
      " 3. f19\n",
      " 4. f21\n",
      " 5. f16\n",
      " 6. f20\n",
      " 7. f17\n",
      " 8. f114\n",
      " 9. f115\n",
      "10. f221\n",
      "Remaining -999 values: 58955751\n",
      "Data types: {dtype('float32'): 364, dtype('O'): 13, dtype('int32'): 3, string[python]: 2, dtype('<M8[ns]'): 2, dtype('int64'): 2}\n",
      "\n",
      "Top 10 selected features:\n",
      " 1. f118\n",
      " 2. f18\n",
      " 3. f19\n",
      " 4. f21\n",
      " 5. f16\n",
      " 6. f20\n",
      " 7. f17\n",
      " 8. f114\n",
      " 9. f115\n",
      "10. f221\n"
     ]
    }
   ],
   "source": [
    "# Stage 3: Advanced feature engineering with robust error handling\n",
    "print(\"\\n=== STAGE 3: ADVANCED FEATURE ENGINEERING ===\")\n",
    "try:\n",
    "    # Validate input data before feature engineering\n",
    "    print(\"Validating input data...\")\n",
    "    input_valid = validate_pipeline_data(cleaned_data['train'], \"Pre-Feature Engineering\")\n",
    "    \n",
    "    if not input_valid:\n",
    "        print(\"⚠️  Input data has issues, performing pre-processing cleanup...\")\n",
    "        numeric_cols = cleaned_data['train'].select_dtypes(include=[np.number]).columns\n",
    "        cleaned_data['train'][numeric_cols] = cleaned_data['train'][numeric_cols].fillna(0)\n",
    "        cleaned_data['train'][numeric_cols] = cleaned_data['train'][numeric_cols].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    print(\"Starting feature engineering...\")\n",
    "    train_engineered, selected_features = create_full_feature_set_advanced(cleaned_data['train'])\n",
    "    test_engineered, _ = create_full_feature_set_advanced(cleaned_data['test'])\n",
    "    \n",
    "    print(f\"✅ Feature engineering completed\")\n",
    "    print(f\"Final train shape: {train_engineered.shape}\")\n",
    "    print(f\"Selected features: {len(selected_features)}\")\n",
    "    \n",
    "    validate_pipeline_data(train_engineered, \"Engineered Train Data\")\n",
    "    \n",
    "    print(\"\\n=== Feature Engineering Quality Report ===\")\n",
    "    numeric_cols = train_engineered.select_dtypes(include=[np.number]).columns\n",
    "    missing_indicator_count = (train_engineered[numeric_cols] == -999).sum().sum()\n",
    "    print(f\"Remaining -999 values: {missing_indicator_count}\")\n",
    "    print(f\"Data types: {train_engineered.dtypes.value_counts().to_dict()}\")\n",
    "    \n",
    "    if selected_features:\n",
    "        print(f\"\\nTop 10 selected features:\")\n",
    "        for i, feature in enumerate(selected_features[:10]):\n",
    "            print(f\"{i+1:2d}. {feature}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Feature engineering failed: {e}\")\n",
    "    print(\"Using original cleaned data without advanced features...\")\n",
    "    \n",
    "    train_engineered = cleaned_data['train'].copy()\n",
    "    test_engineered = cleaned_data['test'].copy()\n",
    "    selected_features = [col for col in train_engineered.columns if col.startswith('f')][:50]\n",
    "    print(f\"✅ Using {len(selected_features)} basic features as fallback\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "726b66d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 4: FINAL VALIDATION ===\n",
      "\n",
      "=== FINAL PIPELINE VALIDATION (FIXED) ===\n",
      "Train data shape: (770164, 386)\n",
      "Test data shape: (369301, 385)\n",
      "Test data columns: ['id1', 'id2', 'id3', 'id4', 'id5', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28_x', 'f29_x', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152_x', 'f153', 'f154', 'f155', 'f156', 'f157_x', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169_x', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217_x', 'f218', 'f219_x', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f310', 'f311', 'f312', 'f313', 'f314', 'f315', 'f316', 'f317', 'f318', 'f319', 'f320', 'f321', 'f322', 'f323', 'f324', 'f325', 'f326', 'f327', 'f328', 'f329', 'f330', 'f331', 'f332', 'f333', 'f334', 'f335', 'f336', 'f337', 'f338', 'f339', 'f340', 'f341', 'f342', 'f343', 'f344', 'f345', 'f346', 'f347', 'f348', 'f349', 'f350', 'f351', 'f352', 'f353', 'f354', 'f355', 'f356', 'f357', 'f358', 'f359', 'f360', 'f361', 'f362', 'f363', 'f364', 'f365', 'f366', 'customer_segment', 'f28_y', 'f29_y', 'f217_y', 'f219_y', 'f152_y', 'f157_y', 'f169_y', 'ctr_merchant_offer', 'total_spending_30d', 'hour', 'day_of_week', 'is_weekend', 'is_business_hours']\n",
      "Available submission columns: ['id1', 'id2', 'id3', 'id5']\n",
      "Available features for modeling: 100\n",
      "\n",
      "✅ ALL VALIDATION CHECKS PASSED\n",
      "\n",
      "=== PIPELINE SUMMARY ===\n",
      "Status: ✅ READY FOR MODEL TRAINING\n",
      "Final train shape: (770164, 386)\n",
      "Final test shape: (369301, 385)\n",
      "Features for modeling: 100\n",
      "\n",
      "✅ ALL VALIDATION CHECKS PASSED\n",
      "\n",
      "=== PIPELINE SUMMARY ===\n",
      "Status: ✅ READY FOR MODEL TRAINING\n",
      "Final train shape: (770164, 386)\n",
      "Final test shape: (369301, 385)\n",
      "Features for modeling: 100\n",
      "Memory usage: 1839.4 MB\n",
      "Memory usage: 1839.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Stage 4: Final validation and summary\n",
    "print(\"\\n=== STAGE 4: FINAL VALIDATION ===\")\n",
    "\n",
    "def final_pipeline_validation_fixed(train_data, test_data, selected_features):\n",
    "    \"\"\"Enhanced validation for pipeline readiness with correct column handling\"\"\"\n",
    "    print(\"\\n=== FINAL PIPELINE VALIDATION (FIXED) ===\")\n",
    "    issues = []\n",
    "    \n",
    "    # Check basic shapes\n",
    "    print(f\"Train data shape: {train_data.shape}\")\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    \n",
    "    # Check available columns in test data\n",
    "    print(f\"Test data columns: {test_data.columns.tolist()}\")\n",
    "    \n",
    "    # Check required columns for submission (based on your template)\n",
    "    required_submit_cols = ['id1', 'id2', 'id3', 'id5']\n",
    "    available_submit_cols = [col for col in required_submit_cols if col in test_data.columns]\n",
    "    missing_submit_cols = [col for col in required_submit_cols if col not in test_data.columns]\n",
    "    \n",
    "    print(f\"Available submission columns: {available_submit_cols}\")\n",
    "    if missing_submit_cols:\n",
    "        print(f\"⚠️ Missing submission columns: {missing_submit_cols}\")\n",
    "        print(\"Will create default values for missing columns\")\n",
    "    \n",
    "    # Check selected features availability\n",
    "    available_features = []\n",
    "    for col in selected_features:\n",
    "        if col in train_data.columns and col in test_data.columns:\n",
    "            available_features.append(col)\n",
    "        else:\n",
    "            issues.append(f\"Feature {col} missing in train or test data\")\n",
    "    \n",
    "    print(f\"Available features for modeling: {len(available_features)}\")\n",
    "    \n",
    "    # Data quality checks\n",
    "    train_nan = train_data[available_features].isna().sum().sum()\n",
    "    test_nan = test_data[available_features].isna().sum().sum()\n",
    "    \n",
    "    if train_nan > 0 or test_nan > 0:\n",
    "        issues.append(f\"NaN values found - Train: {train_nan}, Test: {test_nan}\")\n",
    "    \n",
    "    # Check for infinite values in common numeric columns\n",
    "    train_numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "    test_numeric_cols = test_data.select_dtypes(include=[np.number]).columns\n",
    "    common_numeric_cols = [col for col in train_numeric_cols if col in test_numeric_cols and col != 'y']\n",
    "    \n",
    "    if common_numeric_cols:\n",
    "        train_inf = np.isinf(train_data[common_numeric_cols]).sum().sum()\n",
    "        test_inf = np.isinf(test_data[common_numeric_cols]).sum().sum()\n",
    "        \n",
    "        if train_inf > 0 or test_inf > 0:\n",
    "            issues.append(f\"Infinite values found - Train: {train_inf}, Test: {test_inf}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\n⚠️ VALIDATION ISSUES DETECTED:\")\n",
    "        for i, issue in enumerate(issues, 1):\n",
    "            print(f\"{i}. {issue}\")\n",
    "        return False, available_features\n",
    "    else:\n",
    "        print(\"\\n✅ ALL VALIDATION CHECKS PASSED\")\n",
    "        return True, available_features\n",
    "\n",
    "# Run the fixed validation\n",
    "validation_passed, final_features = final_pipeline_validation_fixed(\n",
    "    train_engineered, test_engineered, selected_features\n",
    ")\n",
    "\n",
    "if not validation_passed:\n",
    "    print(\"\\n🔧 APPLYING FINAL FIXES...\")\n",
    "    # Only fix columns present in both train and test (and skip 'y' in test)\n",
    "    train_numeric_cols = train_engineered.select_dtypes(include=[np.number]).columns\n",
    "    test_numeric_cols = test_engineered.select_dtypes(include=[np.number]).columns\n",
    "    common_numeric_cols = [col for col in train_numeric_cols if col in test_numeric_cols and col != 'y']\n",
    "    train_engineered[common_numeric_cols] = train_engineered[common_numeric_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    test_engineered[common_numeric_cols] = test_engineered[common_numeric_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    validation_passed, final_features = final_pipeline_validation(\n",
    "        train_engineered, test_engineered, selected_features\n",
    "    )\n",
    "\n",
    "print(f\"\\n=== PIPELINE SUMMARY ===\")\n",
    "print(f\"Status: {'✅ READY FOR MODEL TRAINING' if validation_passed else '❌ ISSUES REMAIN'}\")\n",
    "print(f\"Final train shape: {train_engineered.shape}\")\n",
    "print(f\"Final test shape: {test_engineered.shape}\")\n",
    "print(f\"Features for modeling: {len(final_features)}\")\n",
    "print(f\"Memory usage: {(train_engineered.memory_usage(deep=True).sum() / 1024 / 1024):.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0424aa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 5: MODEL TRAINING ===\n",
      "Encoding 4 categorical features with consistent mapping...\n",
      "Encoding 4 categorical features with consistent mapping...\n",
      "Training set: (616131, 100)\n",
      "Validation set: (154033, 100)\n",
      "Training Random Forest model...\n",
      "Training set: (616131, 100)\n",
      "Validation set: (154033, 100)\n",
      "Training Random Forest model...\n",
      "\n",
      "✅ Model training completed!\n",
      "Validation AUC: 0.9241\n",
      "\n",
      "Top 10 most important features:\n",
      "               feature  importance\n",
      "85                f366    0.093896\n",
      "87                f132    0.077656\n",
      "96                f134    0.054268\n",
      "75              f219_y    0.047381\n",
      "91  ctr_merchant_offer    0.046218\n",
      "89                f138    0.043298\n",
      "98                f354    0.038760\n",
      "86                f137    0.037122\n",
      "45                f206    0.036689\n",
      "44                f207    0.035889\n",
      "\n",
      "=== FINAL PREDICTION ON TEST DATA ===\n",
      "\n",
      "✅ Model training completed!\n",
      "Validation AUC: 0.9241\n",
      "\n",
      "Top 10 most important features:\n",
      "               feature  importance\n",
      "85                f366    0.093896\n",
      "87                f132    0.077656\n",
      "96                f134    0.054268\n",
      "75              f219_y    0.047381\n",
      "91  ctr_merchant_offer    0.046218\n",
      "89                f138    0.043298\n",
      "98                f354    0.038760\n",
      "86                f137    0.037122\n",
      "45                f206    0.036689\n",
      "44                f207    0.035889\n",
      "\n",
      "=== FINAL PREDICTION ON TEST DATA ===\n",
      "Test predictions generated. Shape: (369301,)\n",
      "Prediction range: [0.0042, 0.9548]\n",
      "\n",
      "🎯 Use these predictions for Kaggle/test set submission only. Do not use for validation!\n",
      "\n",
      "🎉 PIPELINE EXECUTION COMPLETED!\n",
      "Test predictions generated. Shape: (369301,)\n",
      "Prediction range: [0.0042, 0.9548]\n",
      "\n",
      "🎯 Use these predictions for Kaggle/test set submission only. Do not use for validation!\n",
      "\n",
      "🎉 PIPELINE EXECUTION COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "# Stage 5: Model Training (if validation passed)\n",
    "print(\"\\n=== STAGE 5: MODEL TRAINING ===\")\n",
    "\n",
    "# IMPORTANT: Only use train data for model selection/validation. Test data is for final predictions only.\n",
    "if validation_passed and len(final_features) > 0:\n",
    "    try:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.metrics import roc_auc_score, classification_report\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        \n",
    "        X = train_engineered[final_features].copy()\n",
    "        y = train_engineered['y']\n",
    "        \n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        label_encoders = {}\n",
    "        if len(categorical_cols) > 0:\n",
    "            print(f\"Encoding {len(categorical_cols)} categorical features with consistent mapping...\")\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "                label_encoders[col] = le\n",
    "        \n",
    "        # Split only the training data for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set: {X_train.shape}\")\n",
    "        print(f\"Validation set: {X_val.shape}\")\n",
    "        \n",
    "        print(\"Training Random Forest model...\")\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=12,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        val_pred = model.predict_proba(X_val)[:, 1]\n",
    "        auc_score = roc_auc_score(y_val, val_pred)\n",
    "        \n",
    "        print(f\"\\n✅ Model training completed!\")\n",
    "        print(f\"Validation AUC: {auc_score:.4f}\")\n",
    "        \n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': final_features,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 10 most important features:\")\n",
    "        print(feature_importance.head(10))\n",
    "        \n",
    "        # --- FINAL PREDICTION ON TEST DATA ---\n",
    "        print(\"\\n=== FINAL PREDICTION ON TEST DATA ===\")\n",
    "        X_test = test_engineered[final_features].copy()\n",
    "        \n",
    "        # Apply same categorical encoding to test data\n",
    "        if len(categorical_cols) > 0:\n",
    "            for col in categorical_cols:\n",
    "                if col in X_test.columns:\n",
    "                    le = label_encoders[col]\n",
    "                    # Handle unseen categories in test data\n",
    "                    X_test[col] = X_test[col].astype(str)\n",
    "                    test_categories = set(X_test[col].unique())\n",
    "                    train_categories = set(le.classes_)\n",
    "                    unseen_categories = test_categories - train_categories\n",
    "                    \n",
    "                    if unseen_categories:\n",
    "                        print(f\"Warning: Found {len(unseen_categories)} unseen categories in {col}\")\n",
    "                        # Map unseen categories to most frequent training category\n",
    "                        most_frequent = le.classes_[0]  # First class (alphabetically)\n",
    "                        X_test[col] = X_test[col].apply(lambda x: most_frequent if x in unseen_categories else x)\n",
    "                    \n",
    "                    X_test[col] = le.transform(X_test[col])\n",
    "        \n",
    "        test_pred = model.predict_proba(X_test)[:, 1]\n",
    "        print(f\"Test predictions generated. Shape: {test_pred.shape}\")\n",
    "        print(f\"Prediction range: [{test_pred.min():.4f}, {test_pred.max():.4f}]\")\n",
    "        \n",
    "        print(\"\\n🎯 Use these predictions for Kaggle/test set submission only. Do not use for validation!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model training failed: {e}\")\n",
    "        print(\"Pipeline completed data preparation successfully, but model training needs debugging.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️ Skipping model training due to validation issues.\")\n",
    "    print(\"Focus on fixing data quality issues first.\")\n",
    "\n",
    "print(\"\\n🎉 PIPELINE EXECUTION COMPLETED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e337379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GENERATING SUBMISSION FILE ===\n",
      "Generating MAP@7 optimized submission...\n",
      "Validating submission format...\n",
      "✅ Submission format validation passed\n",
      "✅ MAP@7 submission saved as submission.csv\n",
      "Submission shape: (369301, 5)\n",
      "\n",
      "Sample submission (top 10 rows):\n",
      "                                                   id1      id2       id3  \\\n",
      "82037      1000061_62395_16-23_2023-11-05 09:28:07.805  1000061     62395   \n",
      "153767   1000061_5420674_16-23_2023-11-05 09:28:04.153  1000061   5420674   \n",
      "52697     1000061_430736_16-23_2023-11-05 09:28:12.807  1000061    430736   \n",
      "126858    1000061_803982_16-23_2023-11-05 09:47:52.857  1000061    803982   \n",
      "167476  1000061_80107221_16-23_2023-11-05 09:28:12.577  1000061  80107221   \n",
      "240303     1000061_97081_16-23_2023-11-05 09:28:08.795  1000061     97081   \n",
      "279821  1000061_97904824_16-23_2023-11-05 09:28:08.601  1000061  97904824   \n",
      "288480     1000061_31794_16-23_2023-11-05 09:47:54.852  1000061     31794   \n",
      "8135      1000061_403431_16-23_2023-11-05 09:28:10.592  1000061    403431   \n",
      "46630     1000061_944713_16-23_2023-11-05 09:28:12.529  1000061    944713   \n",
      "\n",
      "              id5      pred  \n",
      "82037  2023-11-05  0.025251  \n",
      "153767 2023-11-05  0.025017  \n",
      "52697  2023-11-05  0.024955  \n",
      "126858 2023-11-05  0.024955  \n",
      "167476 2023-11-05  0.024955  \n",
      "240303 2023-11-05  0.024955  \n",
      "279821 2023-11-05  0.024955  \n",
      "288480 2023-11-05  0.024955  \n",
      "8135   2023-11-05  0.024428  \n",
      "46630  2023-11-05  0.024428  \n",
      "\n",
      "✅ Submission file created successfully!\n",
      "Total predictions: 369301\n",
      "Unique customers: 21118\n",
      "Average predictions per customer: 17.49\n",
      "\n",
      "=== MAP@7 OPTIMIZATION REPORT ===\n",
      "Customers with offers: 21118\n",
      "Max offers per customer: 349\n",
      "Min offers per customer: 1\n",
      "Customers with >=7 offers: 5179\n",
      "✅ MAP@7 submission saved as submission.csv\n",
      "Submission shape: (369301, 5)\n",
      "\n",
      "Sample submission (top 10 rows):\n",
      "                                                   id1      id2       id3  \\\n",
      "82037      1000061_62395_16-23_2023-11-05 09:28:07.805  1000061     62395   \n",
      "153767   1000061_5420674_16-23_2023-11-05 09:28:04.153  1000061   5420674   \n",
      "52697     1000061_430736_16-23_2023-11-05 09:28:12.807  1000061    430736   \n",
      "126858    1000061_803982_16-23_2023-11-05 09:47:52.857  1000061    803982   \n",
      "167476  1000061_80107221_16-23_2023-11-05 09:28:12.577  1000061  80107221   \n",
      "240303     1000061_97081_16-23_2023-11-05 09:28:08.795  1000061     97081   \n",
      "279821  1000061_97904824_16-23_2023-11-05 09:28:08.601  1000061  97904824   \n",
      "288480     1000061_31794_16-23_2023-11-05 09:47:54.852  1000061     31794   \n",
      "8135      1000061_403431_16-23_2023-11-05 09:28:10.592  1000061    403431   \n",
      "46630     1000061_944713_16-23_2023-11-05 09:28:12.529  1000061    944713   \n",
      "\n",
      "              id5      pred  \n",
      "82037  2023-11-05  0.025251  \n",
      "153767 2023-11-05  0.025017  \n",
      "52697  2023-11-05  0.024955  \n",
      "126858 2023-11-05  0.024955  \n",
      "167476 2023-11-05  0.024955  \n",
      "240303 2023-11-05  0.024955  \n",
      "279821 2023-11-05  0.024955  \n",
      "288480 2023-11-05  0.024955  \n",
      "8135   2023-11-05  0.024428  \n",
      "46630  2023-11-05  0.024428  \n",
      "\n",
      "✅ Submission file created successfully!\n",
      "Total predictions: 369301\n",
      "Unique customers: 21118\n",
      "Average predictions per customer: 17.49\n",
      "\n",
      "=== MAP@7 OPTIMIZATION REPORT ===\n",
      "Customers with offers: 21118\n",
      "Max offers per customer: 349\n",
      "Min offers per customer: 1\n",
      "Customers with >=7 offers: 5179\n"
     ]
    }
   ],
   "source": [
    "# --- GENERATE SUBMISSION FILE ---\n",
    "print(\"\\n=== GENERATING SUBMISSION FILE ===\")\n",
    "\n",
    "def validate_submission_format(submission_df, required_cols=['id1', 'id2', 'id3', 'id5', 'pred']):\n",
    "    \"\"\"Validate submission file format\"\"\"\n",
    "    print(\"Validating submission format...\")\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check required columns\n",
    "    missing_cols = [col for col in required_cols if col not in submission_df.columns]\n",
    "    if missing_cols:\n",
    "        issues.append(f\"Missing columns: {missing_cols}\")\n",
    "    \n",
    "    # Check for null values\n",
    "    null_counts = submission_df.isnull().sum()\n",
    "    if null_counts.sum() > 0:\n",
    "        issues.append(f\"Null values found: {null_counts.to_dict()}\")\n",
    "    \n",
    "    # Check prediction range\n",
    "    if 'pred' in submission_df.columns:\n",
    "        pred_min = submission_df['pred'].min()\n",
    "        pred_max = submission_df['pred'].max()\n",
    "        if pred_min < 0 or pred_max > 1:\n",
    "            issues.append(f\"Predictions out of range [0,1]: min={pred_min:.4f}, max={pred_max:.4f}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"⚠️ Submission validation issues:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"✅ Submission format validation passed\")\n",
    "        return True\n",
    "\n",
    "def generate_map7_submission(test_data, predictions, output_path=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Generate submission file optimized for MAP@7 evaluation\n",
    "    \"\"\"\n",
    "    print(\"Generating MAP@7 optimized submission...\")\n",
    "    \n",
    "    # Create submission DataFrame with required columns\n",
    "    submission = pd.DataFrame()\n",
    "    \n",
    "    # Map columns according to submission template\n",
    "    # Use test data columns if they exist, otherwise create defaults\n",
    "    submission['id1'] = test_data.get('id1', range(len(predictions)))\n",
    "    submission['id2'] = test_data.get('id2', range(len(predictions)))\n",
    "    submission['id3'] = test_data.get('id3', range(len(predictions))) \n",
    "    submission['id5'] = test_data.get('id5', pd.Timestamp.now())\n",
    "    submission['pred'] = predictions\n",
    "    \n",
    "    # Sort by customer (id2) and prediction score for MAP@7 optimization\n",
    "    submission = submission.sort_values(['id2', 'pred'], ascending=[True, False])\n",
    "    \n",
    "    # Validate submission format\n",
    "    is_valid = validate_submission_format(submission)\n",
    "    \n",
    "    if not is_valid:\n",
    "        print(\"⚠️ Attempting to fix submission format issues...\")\n",
    "        # Fix any issues\n",
    "        submission = submission.fillna(0)\n",
    "        submission['pred'] = np.clip(submission['pred'], 0, 1)\n",
    "    \n",
    "    # Save submission\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"✅ MAP@7 submission saved as {output_path}\")\n",
    "    print(f\"Submission shape: {submission.shape}\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSample submission (top 10 rows):\")\n",
    "    print(submission.head(10))\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# Generate submission if we have predictions\n",
    "if 'test_pred' in locals() and 'model' in locals():\n",
    "    try:\n",
    "        # Generate the submission file\n",
    "        submission = generate_map7_submission(\n",
    "            test_data=test_engineered,\n",
    "            predictions=test_pred,\n",
    "            output_path=\"submission.csv\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n✅ Submission file created successfully!\")\n",
    "        print(f\"Total predictions: {len(test_pred)}\")\n",
    "        print(f\"Unique customers: {submission['id2'].nunique()}\")\n",
    "        print(f\"Average predictions per customer: {len(submission) / submission['id2'].nunique():.2f}\")\n",
    "        \n",
    "        # Additional MAP@7 optimization\n",
    "        print(\"\\n=== MAP@7 OPTIMIZATION REPORT ===\")\n",
    "        customer_offer_counts = submission.groupby('id2').size()\n",
    "        print(f\"Customers with offers: {len(customer_offer_counts)}\")\n",
    "        print(f\"Max offers per customer: {customer_offer_counts.max()}\")\n",
    "        print(f\"Min offers per customer: {customer_offer_counts.min()}\")\n",
    "        print(f\"Customers with >=7 offers: {(customer_offer_counts >= 7).sum()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Submission generation failed: {e}\")\n",
    "        print(\"Creating emergency submission...\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Emergency submission\n",
    "        try:\n",
    "            emergency_submission = pd.DataFrame({\n",
    "                'id1': [f\"emergency_{i}\" for i in range(len(test_pred))],\n",
    "                'id2': range(len(test_pred)),\n",
    "                'id3': range(len(test_pred)),\n",
    "                'id5': pd.Timestamp.now(),\n",
    "                'pred': test_pred\n",
    "            })\n",
    "            \n",
    "            emergency_submission.to_csv(\"emergency_submission.csv\", index=False)\n",
    "            print(\"✅ Emergency submission saved as emergency_submission.csv\")\n",
    "            \n",
    "        except Exception as emergency_error:\n",
    "            print(f\"❌ Emergency submission also failed: {emergency_error}\")\n",
    "            \n",
    "else:\n",
    "    print(\"❌ No predictions available. Model training may have failed.\")\n",
    "    print(\"Please check the model training section above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d1383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample submission format:\n",
      "                                           id1      id2    id3        id5  \\\n",
      "0  1000061_31794_16-23_2023-11-05 09:47:54.852  1000061  31794  11/5/2023   \n",
      "1  1000061_16099_16-23_2023-11-05 09:28:11.514  1000061  16099  11/5/2023   \n",
      "\n",
      "   pred  \n",
      "0  0.85  \n",
      "1  0.72  \n"
     ]
    }
   ],
   "source": [
    "# Additional utility functions for better submission handling\n",
    "\n",
    "def optimize_for_map7(predictions_df, customer_col='id2', offer_col='id3', pred_col='pred', top_k=7):\n",
    "    \"\"\"\n",
    "    Optimize predictions for MAP@7 by ensuring each customer has top-k ranked offers\n",
    "    \"\"\"\n",
    "    print(f\"Optimizing predictions for MAP@{top_k}...\")\n",
    "    \n",
    "    # Sort by customer and prediction score\n",
    "    sorted_df = predictions_df.sort_values([customer_col, pred_col], ascending=[True, False])\n",
    "    \n",
    "    # Keep only top-k predictions per customer\n",
    "    optimized_df = sorted_df.groupby(customer_col).head(top_k).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Original predictions: {len(predictions_df)}\")\n",
    "    print(f\"Optimized predictions: {len(optimized_df)}\")\n",
    "    print(f\"Average offers per customer: {len(optimized_df) / predictions_df[customer_col].nunique():.2f}\")\n",
    "    \n",
    "    return optimized_df\n",
    "\n",
    "def create_sample_submission_check():\n",
    "    \"\"\"Create a sample submission to verify format\"\"\"\n",
    "    sample_data = {\n",
    "        'id1': ['1000061_31794_16-23_2023-11-05 09:47:54.852', '1000061_16099_16-23_2023-11-05 09:28:11.514'],\n",
    "        'id2': [1000061, 1000061],\n",
    "        'id3': [31794, 16099], \n",
    "        'id5': ['11/5/2023', '11/5/2023'],\n",
    "        'pred': [0.85, 0.72]\n",
    "    }\n",
    "    \n",
    "    sample_df = pd.DataFrame(sample_data)\n",
    "    print(\"Sample submission format:\")\n",
    "    print(sample_df)\n",
    "    return sample_df\n",
    "\n",
    "# Create sample to verify format\n",
    "sample = create_sample_submission_check()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
