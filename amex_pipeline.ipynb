{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed540d6a",
   "metadata": {},
   "source": [
    "# Amex Pipeline: Staged Execution\n",
    "This notebook allows you to run the Amex pipeline in stages: data loading, cleaning, feature engineering, EDA, feature selection, model training, validation, and submission generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05a78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data.data_loader import load_all_data\n",
    "from data.data_cleaning import clean_all_data_advanced\n",
    "from data.advanced_feature_engineering import create_full_feature_set_advanced\n",
    "from eda.exploratory_analysis import (\n",
    "    plot_target_distribution, plot_missing_values, plot_feature_distributions, \n",
    "    plot_correlation_heatmap, plot_new_feature_analysis\n",
    ")\n",
    "from utils.metrics import map7_from_dataframe\n",
    "from utils.submission import generate_submission\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def validate_pipeline_data(data, stage_name):\n",
    "    \"\"\"Validate data at each pipeline stage\"\"\"\n",
    "    print(f\"\\n=== {stage_name} Validation ===\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    \n",
    "    # Check for NaN values\n",
    "    nan_count = data.isna().sum().sum()\n",
    "    print(f\"NaN values: {nan_count}\")\n",
    "    \n",
    "    # Check for infinite values\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    inf_count = np.isinf(data[numeric_cols]).sum().sum()\n",
    "    print(f\"Infinite values: {inf_count}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    memory_mb = data.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    print(f\"Memory usage: {memory_mb:.1f} MB\")\n",
    "    \n",
    "    if nan_count > 0 or inf_count > 0:\n",
    "        print(\"⚠️  Data quality issues detected!\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"✅ Data validation passed\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a46ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 1: DATA LOADING ===\n",
      "✅ Data loaded successfully\n",
      "Train shape: (770164, 372)\n",
      "Test shape: (369301, 371)\n",
      "\n",
      "=== Raw Train Data Validation ===\n",
      "Shape: (770164, 372)\n",
      "NaN values: 68296406\n",
      "Infinite values: 0\n",
      "Memory usage: 1084.8 MB\n",
      "⚠️  Data quality issues detected!\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Load data with validation\n",
    "print(\"=== STAGE 1: DATA LOADING ===\")\n",
    "try:\n",
    "    data = load_all_data()\n",
    "    print(f\"✅ Data loaded successfully\")\n",
    "    print(f\"Train shape: {data['train'].shape}\")\n",
    "    print(f\"Test shape: {data['test'].shape}\")\n",
    "    \n",
    "    # Validate loaded data\n",
    "    validate_pipeline_data(data['train'], \"Raw Train Data\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Data loading failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e418c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 2: ADVANCED DATA CLEANING ===\n",
      "Starting robust advanced data cleaning pipeline...\n",
      "Starting robust advanced data cleaning...\n",
      "Removed 0 duplicate rows\n",
      "Creating focused customer behavioral features with priority features...\n",
      "Creating priority customer aggregations...\n",
      "Created customer aggregations for 7 priority features\n",
      "Creating customer segments with priority features...\n",
      "Created 3 customer segments with 8 priority features\n",
      "Performing focused imputation with priority features...\n",
      "Phase 1: Priority features imputation...\n",
      "Phase 2: Simplified imputation for remaining features...\n",
      "Applying focused imputation strategy...\n",
      "Imputing 491658 missing values in f1\n",
      "Imputing 447192 missing values in f2\n",
      "Imputing 661602 missing values in f3\n",
      "Imputing 701295 missing values in f4\n",
      "Imputing 231810 missing values in f5\n",
      "Imputing 150109 missing values in f6\n",
      "Imputing 367438 missing values in f7\n",
      "Imputing 234515 missing values in f8\n",
      "Imputing 284258 missing values in f9\n",
      "Imputing 253665 missing values in f10\n",
      "Imputing 356562 missing values in f11\n",
      "Imputing 242372 missing values in f12\n",
      "Imputing 769468 missing values in f13\n",
      "Imputing 769468 missing values in f14\n",
      "Imputing 769468 missing values in f15\n",
      "Phase 3: Minimal categorical feature imputation...\n",
      "Robust advanced cleaning completed. Final shape: (770164, 380)\n",
      "Starting robust advanced data cleaning...\n",
      "Removed 0 duplicate rows\n",
      "Creating focused customer behavioral features with priority features...\n",
      "Creating priority customer aggregations...\n",
      "Created customer aggregations for 7 priority features\n",
      "Creating customer segments with priority features...\n",
      "Created 3 customer segments with 8 priority features\n",
      "Performing focused imputation with priority features...\n",
      "Phase 1: Priority features imputation...\n",
      "Phase 2: Simplified imputation for remaining features...\n",
      "Applying focused imputation strategy...\n",
      "Imputing 237422 missing values in f1\n",
      "Imputing 214285 missing values in f2\n",
      "Imputing 319554 missing values in f3\n",
      "Imputing 338807 missing values in f4\n",
      "Imputing 112287 missing values in f5\n",
      "Imputing 74264 missing values in f6\n",
      "Imputing 176785 missing values in f7\n",
      "Imputing 116271 missing values in f8\n",
      "Imputing 139140 missing values in f9\n",
      "Imputing 119899 missing values in f10\n",
      "Imputing 171966 missing values in f11\n",
      "Imputing 116307 missing values in f12\n",
      "Imputing 368853 missing values in f13\n",
      "Imputing 368853 missing values in f14\n",
      "Imputing 368853 missing values in f15\n",
      "Phase 3: Minimal categorical feature imputation...\n",
      "Robust advanced cleaning completed. Final shape: (369301, 379)\n",
      "✅ Advanced cleaning completed\n",
      "Cleaned train shape: (770164, 380)\n",
      "\n",
      "=== Cleaned Train Data Validation ===\n",
      "Shape: (770164, 380)\n",
      "NaN values: 0\n",
      "Infinite values: 0\n",
      "Memory usage: 1815.9 MB\n",
      "✅ Data validation passed\n"
     ]
    }
   ],
   "source": [
    "# Stage 2: Advanced cleaning with comprehensive error handling\n",
    "print(\"\\n=== STAGE 2: ADVANCED DATA CLEANING ===\")\n",
    "try:\n",
    "    cleaned_data = clean_all_data_advanced(data)\n",
    "    print(f\"✅ Advanced cleaning completed\")\n",
    "    print(f\"Cleaned train shape: {cleaned_data['train'].shape}\")\n",
    "    \n",
    "    # Validate cleaned data\n",
    "    is_valid = validate_pipeline_data(cleaned_data['train'], \"Cleaned Train Data\")\n",
    "    \n",
    "    if not is_valid:\n",
    "        print(\"⚠️  Performing emergency data cleanup...\")\n",
    "        # Emergency cleanup\n",
    "        numeric_cols = cleaned_data['train'].select_dtypes(include=[np.number]).columns\n",
    "        cleaned_data['train'][numeric_cols] = cleaned_data['train'][numeric_cols].fillna(0)\n",
    "        cleaned_data['train'][numeric_cols] = cleaned_data['train'][numeric_cols].replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        cleaned_data['test'][numeric_cols] = cleaned_data['test'][numeric_cols].fillna(0)\n",
    "        cleaned_data['test'][numeric_cols] = cleaned_data['test'][numeric_cols].replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        print(\"✅ Emergency cleanup completed\")\n",
    "        validate_pipeline_data(cleaned_data['train'], \"Emergency Cleaned Data\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Data cleaning failed: {e}\")\n",
    "    print(\"Falling back to basic cleaning...\")\n",
    "    \n",
    "    # Emergency fallback\n",
    "    cleaned_data = {\n",
    "        'train': data['train'].fillna(0),\n",
    "        'test': data['test'].fillna(0)\n",
    "    }\n",
    "    print(\"✅ Basic cleaning completed as fallback\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9247b0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 3: ADVANCED FEATURE ENGINEERING ===\n",
      "Validating input data...\n",
      "\n",
      "=== Pre-Feature Engineering Validation ===\n",
      "Shape: (770164, 380)\n",
      "NaN values: 0\n",
      "Infinite values: 0\n",
      "Memory usage: 1815.9 MB\n",
      "✅ Data validation passed\n",
      "Starting feature engineering...\n",
      "Starting advanced feature engineering...\n",
      "\n",
      "=== Data Quality Check: Initial Input ===\n",
      "Memory usage: 1815.9 MB\n",
      "✓ Data quality check passed for Initial Input\n",
      "Creating interaction features...\n",
      "Created 2 interaction features\n",
      "\n",
      "=== Data Quality Check: After Interaction Features ===\n",
      "Memory usage: 1821.8 MB\n",
      "✓ Data quality check passed for After Interaction Features\n",
      "Creating temporal features...\n",
      "Created 4 temporal features\n",
      "\n",
      "=== Data Quality Check: After Temporal Features ===\n",
      "Memory usage: 1839.4 MB\n",
      "✓ Data quality check passed for After Temporal Features\n",
      "Creating aggregated features...\n",
      "Created 0 aggregated features\n",
      "\n",
      "=== Data Quality Check: After Aggregated Features ===\n",
      "Memory usage: 1839.4 MB\n",
      "✓ Data quality check passed for After Aggregated Features\n",
      "Performing emergency NaN cleanup...\n",
      "Performing feature selection...\n",
      "\n",
      "=== Data Quality Check: Feature Selection Input ===\n",
      "Memory usage: 1839.4 MB\n",
      "✓ Data quality check passed for Feature Selection Input\n",
      "Performing emergency NaN cleanup...\n",
      "Selected top 100 features\n",
      "Top 10 features:\n",
      "    feature  importance\n",
      "117    f118    0.209120\n",
      "17      f18    0.206758\n",
      "18      f19    0.206305\n",
      "20      f21    0.205947\n",
      "15      f16    0.205752\n",
      "19      f20    0.205453\n",
      "16      f17    0.205097\n",
      "113    f114    0.202030\n",
      "114    f115    0.183089\n",
      "220    f221    0.180139\n",
      "\n",
      "=== Data Quality Check: Final Output ===\n",
      "Memory usage: 1839.4 MB\n",
      "✓ Data quality check passed for Final Output\n",
      "Advanced feature engineering completed.\n",
      "Total features created: 6\n",
      "Selected features: 100\n",
      "Starting advanced feature engineering...\n",
      "\n",
      "=== Data Quality Check: Initial Input ===\n",
      "Memory usage: 867.0 MB\n",
      "✓ Data quality check passed for Initial Input\n",
      "Creating interaction features...\n",
      "Created 2 interaction features\n",
      "\n",
      "=== Data Quality Check: After Interaction Features ===\n",
      "Memory usage: 869.8 MB\n",
      "✓ Data quality check passed for After Interaction Features\n",
      "Creating temporal features...\n",
      "Created 4 temporal features\n",
      "\n",
      "=== Data Quality Check: After Temporal Features ===\n",
      "Memory usage: 878.3 MB\n",
      "✓ Data quality check passed for After Temporal Features\n",
      "Creating aggregated features...\n",
      "Created 0 aggregated features\n",
      "\n",
      "=== Data Quality Check: After Aggregated Features ===\n",
      "Memory usage: 878.3 MB\n",
      "✓ Data quality check passed for After Aggregated Features\n",
      "Performing emergency NaN cleanup...\n",
      "Performing feature selection...\n",
      "\n",
      "=== Data Quality Check: Feature Selection Input ===\n",
      "Memory usage: 878.3 MB\n",
      "✓ Data quality check passed for Feature Selection Input\n",
      "Target column not found, returning all features\n",
      "\n",
      "=== Data Quality Check: Final Output ===\n",
      "Memory usage: 878.3 MB\n",
      "✓ Data quality check passed for Final Output\n",
      "Advanced feature engineering completed.\n",
      "Total features created: 6\n",
      "Selected features: 379\n",
      "✅ Feature engineering completed\n",
      "Final train shape: (770164, 386)\n",
      "Selected features: 100\n",
      "\n",
      "=== Engineered Train Data Validation ===\n",
      "Shape: (770164, 386)\n",
      "NaN values: 0\n",
      "Infinite values: 0\n",
      "Memory usage: 1839.4 MB\n",
      "✅ Data validation passed\n",
      "\n",
      "=== Feature Engineering Quality Report ===\n",
      "Remaining -999 values: 58955751\n",
      "Data types: {dtype('float32'): 364, dtype('O'): 13, dtype('int32'): 3, string[python]: 2, dtype('<M8[ns]'): 2, dtype('int64'): 2}\n",
      "\n",
      "Top 10 selected features:\n",
      " 1. f118\n",
      " 2. f18\n",
      " 3. f19\n",
      " 4. f21\n",
      " 5. f16\n",
      " 6. f20\n",
      " 7. f17\n",
      " 8. f114\n",
      " 9. f115\n",
      "10. f221\n"
     ]
    }
   ],
   "source": [
    "# Stage 3: Advanced feature engineering with robust error handling\n",
    "print(\"\\n=== STAGE 3: ADVANCED FEATURE ENGINEERING ===\")\n",
    "try:\n",
    "    # Validate input data before feature engineering\n",
    "    print(\"Validating input data...\")\n",
    "    input_valid = validate_pipeline_data(cleaned_data['train'], \"Pre-Feature Engineering\")\n",
    "    \n",
    "    if not input_valid:\n",
    "        print(\"⚠️  Input data has issues, performing pre-processing cleanup...\")\n",
    "        numeric_cols = cleaned_data['train'].select_dtypes(include=[np.number]).columns\n",
    "        cleaned_data['train'][numeric_cols] = cleaned_data['train'][numeric_cols].fillna(0)\n",
    "        cleaned_data['train'][numeric_cols] = cleaned_data['train'][numeric_cols].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    print(\"Starting feature engineering...\")\n",
    "    train_engineered, selected_features = create_full_feature_set_advanced(cleaned_data['train'])\n",
    "    test_engineered, _ = create_full_feature_set_advanced(cleaned_data['test'])\n",
    "    \n",
    "    print(f\"✅ Feature engineering completed\")\n",
    "    print(f\"Final train shape: {train_engineered.shape}\")\n",
    "    print(f\"Selected features: {len(selected_features)}\")\n",
    "    \n",
    "    validate_pipeline_data(train_engineered, \"Engineered Train Data\")\n",
    "    \n",
    "    print(\"\\n=== Feature Engineering Quality Report ===\")\n",
    "    numeric_cols = train_engineered.select_dtypes(include=[np.number]).columns\n",
    "    missing_indicator_count = (train_engineered[numeric_cols] == -999).sum().sum()\n",
    "    print(f\"Remaining -999 values: {missing_indicator_count}\")\n",
    "    print(f\"Data types: {train_engineered.dtypes.value_counts().to_dict()}\")\n",
    "    \n",
    "    if selected_features:\n",
    "        print(f\"\\nTop 10 selected features:\")\n",
    "        for i, feature in enumerate(selected_features[:10]):\n",
    "            print(f\"{i+1:2d}. {feature}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Feature engineering failed: {e}\")\n",
    "    print(\"Using original cleaned data without advanced features...\")\n",
    "    \n",
    "    train_engineered = cleaned_data['train'].copy()\n",
    "    test_engineered = cleaned_data['test'].copy()\n",
    "    selected_features = [col for col in train_engineered.columns if col.startswith('f')][:50]\n",
    "    print(f\"✅ Using {len(selected_features)} basic features as fallback\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "726b66d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 4: FINAL VALIDATION ===\n",
      "Performing comprehensive pipeline validation...\n",
      "Train data shape: (770164, 386)\n",
      "Test data shape: (369301, 385)\n",
      "Target distribution: {0.0: 733113, 1.0: 37051}\n",
      "Available features: 100/100\n",
      "\n",
      "✅ ALL VALIDATION CHECKS PASSED\n",
      "\n",
      "=== PIPELINE SUMMARY ===\n",
      "Status: ✅ READY FOR MODEL TRAINING\n",
      "Final train shape: (770164, 386)\n",
      "Final test shape: (369301, 385)\n",
      "Features for modeling: 100\n",
      "Memory usage: 1839.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Stage 4: Final validation and summary\n",
    "print(\"\\n=== STAGE 4: FINAL VALIDATION ===\")\n",
    "\n",
    "def final_pipeline_validation(train_data, test_data, features):\n",
    "    \"\"\"Comprehensive validation before model training\"\"\"\n",
    "    print(\"Performing comprehensive pipeline validation...\")\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    print(f\"Train data shape: {train_data.shape}\")\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    \n",
    "    if 'y' in train_data.columns:\n",
    "        target_dist = train_data['y'].value_counts()\n",
    "        print(f\"Target distribution: {target_dist.to_dict()}\")\n",
    "        if len(target_dist) < 2:\n",
    "            issues.append(\"Target variable has insufficient classes\")\n",
    "    else:\n",
    "        issues.append(\"Target variable 'y' not found\")\n",
    "    \n",
    "    available_features = [f for f in features if f in train_data.columns]\n",
    "    print(f\"Available features: {len(available_features)}/{len(features)}\")\n",
    "    \n",
    "    if len(available_features) < len(features) * 0.8:\n",
    "        issues.append(f\"Too many features missing: {len(features) - len(available_features)}\")\n",
    "    \n",
    "    train_nan = train_data.isna().sum().sum()\n",
    "    test_nan = test_data.isna().sum().sum()\n",
    "    \n",
    "    if train_nan > 0 or test_nan > 0:\n",
    "        issues.append(f\"NaN values found - Train: {train_nan}, Test: {test_nan}\")\n",
    "    \n",
    "    # Only use numeric columns present in both train and test (and skip 'y' in test)\n",
    "    train_numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "    test_numeric_cols = test_data.select_dtypes(include=[np.number]).columns\n",
    "    common_numeric_cols = [col for col in train_numeric_cols if col in test_numeric_cols and col != 'y']\n",
    "    train_inf = np.isinf(train_data[common_numeric_cols]).sum().sum() if common_numeric_cols else 0\n",
    "    test_inf = np.isinf(test_data[common_numeric_cols]).sum().sum() if common_numeric_cols else 0\n",
    "    \n",
    "    if train_inf > 0 or test_inf > 0:\n",
    "        issues.append(f\"Infinite values found - Train: {train_inf}, Test: {test_inf}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\n⚠️  VALIDATION ISSUES DETECTED:\")\n",
    "        for i, issue in enumerate(issues, 1):\n",
    "            print(f\"{i}. {issue}\")\n",
    "        return False, available_features\n",
    "    else:\n",
    "        print(\"\\n✅ ALL VALIDATION CHECKS PASSED\")\n",
    "        return True, available_features\n",
    "\n",
    "validation_passed, final_features = final_pipeline_validation(\n",
    "    train_engineered, test_engineered, selected_features\n",
    ")\n",
    "\n",
    "if not validation_passed:\n",
    "    print(\"\\n🔧 APPLYING FINAL FIXES...\")\n",
    "    # Only fix columns present in both train and test (and skip 'y' in test)\n",
    "    train_numeric_cols = train_engineered.select_dtypes(include=[np.number]).columns\n",
    "    test_numeric_cols = test_engineered.select_dtypes(include=[np.number]).columns\n",
    "    common_numeric_cols = [col for col in train_numeric_cols if col in test_numeric_cols and col != 'y']\n",
    "    train_engineered[common_numeric_cols] = train_engineered[common_numeric_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    test_engineered[common_numeric_cols] = test_engineered[common_numeric_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    validation_passed, final_features = final_pipeline_validation(\n",
    "        train_engineered, test_engineered, selected_features\n",
    "    )\n",
    "\n",
    "print(f\"\\n=== PIPELINE SUMMARY ===\")\n",
    "print(f\"Status: {'✅ READY FOR MODEL TRAINING' if validation_passed else '❌ ISSUES REMAIN'}\")\n",
    "print(f\"Final train shape: {train_engineered.shape}\")\n",
    "print(f\"Final test shape: {test_engineered.shape}\")\n",
    "print(f\"Features for modeling: {len(final_features)}\")\n",
    "print(f\"Memory usage: {(train_engineered.memory_usage(deep=True).sum() / 1024 / 1024):.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0424aa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 5: MODEL TRAINING ===\n",
      "Encoding 4 categorical features...\n",
      "Training set: (616131, 100)\n",
      "Validation set: (154033, 100)\n",
      "Training Random Forest model...\n",
      "\n",
      "✅ Model training completed!\n",
      "Validation AUC: 0.9165\n",
      "\n",
      "Top 10 most important features:\n",
      "               feature  importance\n",
      "85                f366    0.099298\n",
      "87                f132    0.083531\n",
      "91  ctr_merchant_offer    0.054147\n",
      "89                f138    0.048363\n",
      "75              f219_y    0.048219\n",
      "45                f206    0.048001\n",
      "86                f137    0.047555\n",
      "96                f134    0.044824\n",
      "93                f147    0.041888\n",
      "98                f354    0.038801\n",
      "\n",
      "=== FINAL PREDICTION ON TEST DATA ===\n",
      "Test predictions generated. Shape: (369301,)\n",
      "\n",
      "🎯 Use these predictions for Kaggle/test set submission only. Do not use for validation!\n",
      "\n",
      "🎉 PIPELINE EXECUTION COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "# Stage 5: Model Training (if validation passed)\n",
    "print(\"\\n=== STAGE 5: MODEL TRAINING ===\")\n",
    "\n",
    "# IMPORTANT: Only use train data for model selection/validation. Test data is for final predictions only.\n",
    "if validation_passed and len(final_features) > 0:\n",
    "    try:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.metrics import roc_auc_score, classification_report\n",
    "        \n",
    "        X = train_engineered[final_features].copy()\n",
    "        y = train_engineered['y']\n",
    "        \n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        if len(categorical_cols) > 0:\n",
    "            print(f\"Encoding {len(categorical_cols)} categorical features...\")\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "        \n",
    "        # Split only the training data for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set: {X_train.shape}\")\n",
    "        print(f\"Validation set: {X_val.shape}\")\n",
    "        \n",
    "        print(\"Training Random Forest model...\")\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        val_pred = model.predict_proba(X_val)[:, 1]\n",
    "        auc_score = roc_auc_score(y_val, val_pred)\n",
    "        \n",
    "        print(f\"\\n✅ Model training completed!\")\n",
    "        print(f\"Validation AUC: {auc_score:.4f}\")\n",
    "        \n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': final_features,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 10 most important features:\")\n",
    "        print(feature_importance.head(10))\n",
    "        \n",
    "        # --- FINAL PREDICTION ON TEST DATA ---\n",
    "        print(\"\\n=== FINAL PREDICTION ON TEST DATA ===\")\n",
    "        X_test = test_engineered[final_features].copy()\n",
    "        if len(categorical_cols) > 0:\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                X_test[col] = le.fit_transform(X_test[col].astype(str))\n",
    "        test_pred = model.predict_proba(X_test)[:, 1]\n",
    "        print(f\"Test predictions generated. Shape: {test_pred.shape}\")\n",
    "        # Save or submit predictions as needed\n",
    "        # Example: generate_submission(...)\n",
    "        # generate_submission(...)\n",
    "        print(\"\\n🎯 Use these predictions for Kaggle/test set submission only. Do not use for validation!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model training failed: {e}\")\n",
    "        print(\"Pipeline completed data preparation successfully, but model training needs debugging.\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️  Skipping model training due to validation issues.\")\n",
    "    print(\"Focus on fixing data quality issues first.\")\n",
    "\n",
    "print(\"\\n🎉 PIPELINE EXECUTION COMPLETED!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
