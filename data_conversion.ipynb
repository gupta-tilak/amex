{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dtype mapping for 372 columns\n",
      "Sampling 1000 rows to check for conversion issues...\n",
      "Dataset has 6,339,465 total rows, using 1000 for testing\n",
      "Analyzing NaN impact on dtype conversions...\n",
      "\n",
      "Found 0 columns with NaNs targeting integer types:\n",
      "Recommended NaN strategy: no_action_needed\n",
      "Loading full dataset...\n",
      "Converting dtypes safely with NaN handling...\n",
      "\n",
      "NaN Summary:\n",
      "Total NaN values: 0\n",
      "Columns with NaNs: 0/1\n",
      "\n",
      "Final dtypes:\n",
      "object     8\n",
      "float64    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset shape: (6339465, 9)\n",
      "Memory usage: 2911.49 MB\n",
      "Total NaN values: 1,304\n",
      "Columns still with NaNs: 1\n",
      "Top 10 columns by NaN count:\n",
      "id8    1304\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_automated_dtype_mapping(data_dict_path_or_df):\n",
    "    \"\"\"\n",
    "    Automatically create dtype mapping from data dictionary with 371+ features\n",
    "    \"\"\"\n",
    "    # Load data dictionary if path provided\n",
    "    if isinstance(data_dict_path_or_df, str):\n",
    "        data_dict = pd.read_csv(data_dict_path_or_df)\n",
    "    else:\n",
    "        data_dict = data_dict_path_or_df.copy()\n",
    "    \n",
    "    dtype_mapping = {}\n",
    "    \n",
    "    for _, row in data_dict.iterrows():\n",
    "        column = row['masked_column']\n",
    "        data_type = row['Type']\n",
    "        description = str(row['Description']).lower()\n",
    "        \n",
    "        # Map based on Type column\n",
    "        if data_type == 'Key':\n",
    "            dtype_mapping[column] = 'object'\n",
    "        elif data_type == 'Categorical':\n",
    "            dtype_mapping[column] = 'category'\n",
    "        elif data_type == 'Label':\n",
    "            # Check if binary classification or regression\n",
    "            dtype_mapping[column] = 'int8'  # Assume binary, adjust if needed\n",
    "        elif data_type == 'Numerical':\n",
    "            # Smart mapping based on description\n",
    "            if any(keyword in description for keyword in ['whether', 'binary', 'flag', 'indicator']):\n",
    "                # Binary features (0/1)\n",
    "                dtype_mapping[column] = 'int8'\n",
    "            elif any(keyword in description for keyword in ['count', 'number of', 'quantity']):\n",
    "                # Count features (integers)\n",
    "                dtype_mapping[column] = 'int32'\n",
    "            elif 'score' in description or 'rate' in description or 'ratio' in description:\n",
    "                # Scores and rates (floats)\n",
    "                dtype_mapping[column] = 'float32'\n",
    "            elif 'timestamp' in description or 'time' in description:\n",
    "                # Timestamps\n",
    "                dtype_mapping[column] = 'int64'  # or 'datetime64[ns]'\n",
    "            elif 'date' in description:\n",
    "                # Dates\n",
    "                dtype_mapping[column] = 'object'  # Convert to datetime later\n",
    "            else:\n",
    "                # Default numerical\n",
    "                dtype_mapping[column] = 'float32'\n",
    "        else:\n",
    "            # Unknown type, keep as object\n",
    "            dtype_mapping[column] = 'object'\n",
    "    \n",
    "    return dtype_mapping\n",
    "\n",
    "def load_and_convert_parquet(parquet_path, data_dict_path, sample_first=True, nan_strategy='nullable', sample_size=1000):\n",
    "    \"\"\"\n",
    "    Load parquet file and convert dtypes based on data dictionary with NaN handling\n",
    "    \n",
    "    Parameters:\n",
    "    - nan_strategy: 'nullable', 'float', 'fill_zero', 'fill_mode', 'analyze'\n",
    "    - sample_size: number of rows to sample for testing (if sample_first=True)\n",
    "    \"\"\"\n",
    "    # Create dtype mapping\n",
    "    dtype_mapping = create_automated_dtype_mapping(data_dict_path)\n",
    "    \n",
    "    print(f\"Created dtype mapping for {len(dtype_mapping)} columns\")\n",
    "    \n",
    "    # Sample first to check for issues (optional)\n",
    "    if sample_first:\n",
    "        print(f\"Sampling {sample_size} rows to check for conversion issues...\")\n",
    "        \n",
    "        # More efficient sampling for large files using PyArrow\n",
    "        try:\n",
    "            import pyarrow.parquet as pq\n",
    "            # Read just the first batch/chunk\n",
    "            parquet_file = pq.ParquetFile(parquet_path)\n",
    "            first_batch = parquet_file.read_row_group(0)\n",
    "            df_sample = first_batch.to_pandas().head(sample_size)\n",
    "            total_rows = parquet_file.metadata.num_rows\n",
    "            print(f\"Dataset has {total_rows:,} total rows, using {len(df_sample)} for testing\")\n",
    "        except ImportError:\n",
    "            # Fallback to pandas if PyArrow not available\n",
    "            print(\"PyArrow not available, loading full dataset for sampling...\")\n",
    "            df_full = pd.read_parquet(parquet_path)\n",
    "            df_sample = df_full.head(sample_size)\n",
    "            total_rows = len(df_full)\n",
    "            print(f\"Dataset has {total_rows:,} total rows, using {len(df_sample)} for testing\")\n",
    "        \n",
    "        # Analyze NaN impact\n",
    "        nan_analysis = analyze_nan_impact(df_sample, dtype_mapping)\n",
    "        \n",
    "        if nan_strategy == 'analyze':\n",
    "            nan_strategy = recommend_nan_strategy(nan_analysis)\n",
    "            print(f\"Recommended NaN strategy: {nan_strategy}\")\n",
    "        \n",
    "        # Test conversions on sample\n",
    "        conversion_issues = test_dtype_conversions(df_sample, dtype_mapping)\n",
    "        if conversion_issues:\n",
    "            print(\"Found conversion issues:\")\n",
    "            for col, issue in conversion_issues.items():\n",
    "                print(f\"  {col}: {issue}\")\n",
    "            \n",
    "            # Adjust mapping based on issues\n",
    "            dtype_mapping = fix_conversion_issues(df_sample, dtype_mapping, conversion_issues)\n",
    "    \n",
    "    # Load full dataset\n",
    "    print(\"Loading full dataset...\")\n",
    "    if sample_first and 'df_full' in locals():\n",
    "        df = df_full  # Already loaded for sampling\n",
    "    else:\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "    \n",
    "    # Convert dtypes with NaN handling\n",
    "    df = safe_convert_all_dtypes(df, dtype_mapping, nan_strategy)\n",
    "    \n",
    "    return df, dtype_mapping\n",
    "\n",
    "def test_dtype_conversions(df_sample, dtype_mapping):\n",
    "    \"\"\"\n",
    "    Test dtype conversions on a sample to identify issues\n",
    "    \"\"\"\n",
    "    issues = {}\n",
    "    \n",
    "    for col, target_dtype in dtype_mapping.items():\n",
    "        if col not in df_sample.columns:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            if target_dtype in ['int8', 'int16', 'int32', 'int64']:\n",
    "                pd.to_numeric(df_sample[col], errors='raise').astype(target_dtype)\n",
    "            elif target_dtype in ['float16', 'float32', 'float64']:\n",
    "                pd.to_numeric(df_sample[col], errors='raise').astype(target_dtype)\n",
    "            elif target_dtype == 'category':\n",
    "                df_sample[col].astype('category')\n",
    "        except Exception as e:\n",
    "            issues[col] = str(e)\n",
    "    \n",
    "    return issues\n",
    "\n",
    "def fix_conversion_issues(df_sample, dtype_mapping, issues):\n",
    "    \"\"\"\n",
    "    Automatically fix common conversion issues\n",
    "    \"\"\"\n",
    "    fixed_mapping = dtype_mapping.copy()\n",
    "    \n",
    "    for col, issue in issues.items():\n",
    "        print(f\"Fixing {col}: {issue}\")\n",
    "        \n",
    "        # Check actual data in problematic column\n",
    "        sample_values = df_sample[col].dropna().head(10)\n",
    "        print(f\"Sample values: {sample_values.tolist()}\")\n",
    "        \n",
    "        # Common fixes\n",
    "        if \"cannot convert\" in issue.lower() or \"invalid literal\" in issue.lower():\n",
    "            # Try float first, then object\n",
    "            try:\n",
    "                pd.to_numeric(df_sample[col], errors='coerce')\n",
    "                fixed_mapping[col] = 'float32'\n",
    "                print(f\"  -> Changed to float32\")\n",
    "            except:\n",
    "                fixed_mapping[col] = 'object'\n",
    "                print(f\"  -> Keeping as object\")\n",
    "    \n",
    "    return fixed_mapping\n",
    "\n",
    "def safe_convert_all_dtypes(df, dtype_mapping, handle_nans_strategy='nullable'):\n",
    "    \"\"\"\n",
    "    Safely convert all columns with comprehensive NaN handling\n",
    "    \n",
    "    Parameters:\n",
    "    - handle_nans_strategy: 'nullable', 'float', 'fill_zero', 'fill_mode'\n",
    "    \"\"\"\n",
    "    print(\"Converting dtypes safely with NaN handling...\")\n",
    "    conversion_report = {}\n",
    "    nan_report = {}\n",
    "    \n",
    "    for col, target_dtype in dtype_mapping.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        original_dtype = df[col].dtype\n",
    "        nan_count = df[col].isna().sum()\n",
    "        nan_report[col] = nan_count\n",
    "        \n",
    "        try:\n",
    "            if target_dtype == 'category':\n",
    "                # Categories can handle NaNs naturally\n",
    "                df[col] = df[col].astype('category')\n",
    "                \n",
    "            elif target_dtype in ['int8', 'int16', 'int32', 'int64']:\n",
    "                # Integer types cannot handle NaNs - need special handling\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                \n",
    "                if df[col].isna().any():\n",
    "                    if handle_nans_strategy == 'nullable':\n",
    "                        # Use nullable integer types (pandas >= 0.24)\n",
    "                        nullable_type = target_dtype.replace('int', 'Int')  # Int8, Int32, etc.\n",
    "                        df[col] = df[col].astype(nullable_type)\n",
    "                        print(f\"  {col}: Using nullable {nullable_type} due to {nan_count} NaNs\")\n",
    "                    elif handle_nans_strategy == 'float':\n",
    "                        # Convert to float instead (can handle NaNs)\n",
    "                        float_type = target_dtype.replace('int', 'float')\n",
    "                        df[col] = df[col].astype(float_type)\n",
    "                        print(f\"  {col}: Converted to {float_type} due to {nan_count} NaNs\")\n",
    "                    elif handle_nans_strategy == 'fill_zero':\n",
    "                        # Fill NaNs with 0\n",
    "                        df[col] = df[col].fillna(0).astype(target_dtype)\n",
    "                        print(f\"  {col}: Filled {nan_count} NaNs with 0\")\n",
    "                    elif handle_nans_strategy == 'fill_mode':\n",
    "                        # Fill with mode (most common value)\n",
    "                        mode_val = df[col].mode().iloc[0] if not df[col].mode().empty else 0\n",
    "                        df[col] = df[col].fillna(mode_val).astype(target_dtype)\n",
    "                        print(f\"  {col}: Filled {nan_count} NaNs with mode value {mode_val}\")\n",
    "                else:\n",
    "                    df[col] = df[col].astype(target_dtype)\n",
    "                    \n",
    "            elif target_dtype in ['float16', 'float32', 'float64']:\n",
    "                # Float types can handle NaNs naturally\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').astype(target_dtype)\n",
    "                if nan_count > 0:\n",
    "                    print(f\"  {col}: Keeping {nan_count} NaN values in {target_dtype}\")\n",
    "                    \n",
    "            else:\n",
    "                # Object and other types\n",
    "                df[col] = df[col].astype(target_dtype)\n",
    "                \n",
    "            conversion_report[col] = f\"{original_dtype} -> {df[col].dtype} ✓\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            conversion_report[col] = f\"{original_dtype} -> {target_dtype} ✗ ({str(e)})\"\n",
    "            print(f\"Failed to convert {col}: {e}\")\n",
    "    \n",
    "    # Summary of NaN handling\n",
    "    print(f\"\\nNaN Summary:\")\n",
    "    total_nans = sum(nan_report.values())\n",
    "    cols_with_nans = sum(1 for count in nan_report.values() if count > 0)\n",
    "    print(f\"Total NaN values: {total_nans:,}\")\n",
    "    print(f\"Columns with NaNs: {cols_with_nans}/{len(nan_report)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_nan_impact(df, dtype_mapping):\n",
    "    \"\"\"\n",
    "    Analyze how NaNs will affect dtype conversions\n",
    "    \"\"\"\n",
    "    print(\"Analyzing NaN impact on dtype conversions...\")\n",
    "    \n",
    "    nan_analysis = {}\n",
    "    problematic_cols = []\n",
    "    \n",
    "    for col, target_dtype in dtype_mapping.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        nan_count = df[col].isna().sum()\n",
    "        nan_pct = (nan_count / len(df)) * 100\n",
    "        \n",
    "        # Check if this will be problematic\n",
    "        is_problematic = (\n",
    "            nan_count > 0 and \n",
    "            target_dtype in ['int8', 'int16', 'int32', 'int64']\n",
    "        )\n",
    "        \n",
    "        nan_analysis[col] = {\n",
    "            'nan_count': nan_count,\n",
    "            'nan_percentage': nan_pct,\n",
    "            'target_dtype': target_dtype,\n",
    "            'problematic': is_problematic\n",
    "        }\n",
    "        \n",
    "        if is_problematic:\n",
    "            problematic_cols.append(col)\n",
    "    \n",
    "    print(f\"\\nFound {len(problematic_cols)} columns with NaNs targeting integer types:\")\n",
    "    for col in problematic_cols[:10]:  # Show first 10\n",
    "        info = nan_analysis[col]\n",
    "        print(f\"  {col}: {info['nan_count']} NaNs ({info['nan_percentage']:.1f}%) -> {info['target_dtype']}\")\n",
    "    \n",
    "    if len(problematic_cols) > 10:\n",
    "        print(f\"  ... and {len(problematic_cols) - 10} more\")\n",
    "    \n",
    "    return nan_analysis\n",
    "\n",
    "def recommend_nan_strategy(nan_analysis):\n",
    "    \"\"\"\n",
    "    Recommend best NaN handling strategy based on data\n",
    "    \"\"\"\n",
    "    integer_cols_with_nans = [\n",
    "        col for col, info in nan_analysis.items() \n",
    "        if info['problematic']\n",
    "    ]\n",
    "    \n",
    "    if not integer_cols_with_nans:\n",
    "        return 'no_action_needed'\n",
    "    \n",
    "    # Calculate statistics\n",
    "    high_nan_cols = [\n",
    "        col for col in integer_cols_with_nans \n",
    "        if nan_analysis[col]['nan_percentage'] > 20\n",
    "    ]\n",
    "    \n",
    "    binary_like_cols = [\n",
    "        col for col in integer_cols_with_nans \n",
    "        if nan_analysis[col]['target_dtype'] == 'int8'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nRecommendations:\")\n",
    "    print(f\"- {len(high_nan_cols)} columns have >20% NaNs - consider 'nullable' or 'float'\")\n",
    "    print(f\"- {len(binary_like_cols)} binary columns with NaNs - consider 'fill_zero' or 'nullable'\")\n",
    "    \n",
    "    if len(high_nan_cols) > len(integer_cols_with_nans) * 0.5:\n",
    "        return 'nullable'  # Many high-NaN columns\n",
    "    elif len(binary_like_cols) > len(integer_cols_with_nans) * 0.7:\n",
    "        return 'fill_zero'  # Mostly binary features\n",
    "    else:\n",
    "        return 'nullable'  # Safe default\n",
    "\n",
    "def analyze_memory_savings(df_before, df_after):\n",
    "    \"\"\"\n",
    "    Analyze memory savings after dtype conversion\n",
    "    \"\"\"\n",
    "    mem_before = df_before.memory_usage(deep=True).sum() / 1024**2\n",
    "    mem_after = df_after.memory_usage(deep=True).sum() / 1024**2\n",
    "    savings = mem_before - mem_after\n",
    "    \n",
    "    print(f\"\\nMemory Analysis:\")\n",
    "    print(f\"Before conversion: {mem_before:.2f} MB\")\n",
    "    print(f\"After conversion:  {mem_after:.2f} MB\")\n",
    "    print(f\"Memory saved:      {savings:.2f} MB ({savings/mem_before*100:.1f}%)\")\n",
    "\n",
    "# Usage examples with different NaN strategies\n",
    "if __name__ == \"__main__\":\n",
    "    data_dict_path = \"dataset/data_dictionary.csv\"\n",
    "    parquet_path = \"dataset/add_trans.parquet\"\n",
    "    \n",
    "    # Method 1: Auto-analyze and recommend strategy\n",
    "    df, dtype_mapping = load_and_convert_parquet(\n",
    "        parquet_path, data_dict_path, nan_strategy='analyze'\n",
    "    )\n",
    "    \n",
    "    # Method 2: Use nullable integers (recommended for most cases)\n",
    "    # df, dtype_mapping = load_and_convert_parquet(\n",
    "    #     parquet_path, data_dict_path, nan_strategy='nullable'\n",
    "    # )\n",
    "    \n",
    "    # Method 3: Convert problematic integers to floats\n",
    "    # df, dtype_mapping = load_and_convert_parquet(\n",
    "    #     parquet_path, data_dict_path, nan_strategy='float'\n",
    "    # )\n",
    "    \n",
    "    # Method 4: Fill NaNs with 0 (good for binary features)\n",
    "    # df, dtype_mapping = load_and_convert_parquet(\n",
    "    #     parquet_path, data_dict_path, nan_strategy='fill_zero'\n",
    "    # )\n",
    "    \n",
    "    print(\"\\nFinal dtypes:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"Total NaN values: {df.isna().sum().sum():,}\")\n",
    "    \n",
    "    # Check specific columns with NaNs\n",
    "    cols_with_nans = df.columns[df.isna().any()].tolist()\n",
    "    print(f\"Columns still with NaNs: {len(cols_with_nans)}\")\n",
    "    \n",
    "    if cols_with_nans:\n",
    "        print(\"Top 10 columns by NaN count:\")\n",
    "        nan_counts = df[cols_with_nans].isna().sum().sort_values(ascending=False)\n",
    "        print(nan_counts.head(10))\n",
    "    \n",
    "    # Save the optimized dataset\n",
    "    # df.to_parquet(\"optimized_data.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('dataset/processed/add_trans_processed.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
